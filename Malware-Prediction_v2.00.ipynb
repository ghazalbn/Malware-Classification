{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVPQiCMclXTQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y13TRKtRG73P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import load_model\n",
        "from keras import layers\n",
        "from keras.regularizers import L1, L2, L1L2\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1qQWVVlamQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znCofgxFlfbz"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEZn8zdt530L",
        "outputId": "c0f33513-287d-42cc-95e4-98bde5436c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MZQyevkz8w0XpecWUQu98v3BKaWIgho0\n",
            "To: /content/benign.txt\n",
            "100% 151M/151M [00:03<00:00, 42.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dlPq2KgrTWJdopN96pi0LkU8BPaCld1N\n",
            "To: /content/maleware.txt\n",
            "100% 343M/343M [00:05<00:00, 58.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7zEKtg2cMqo0s1SfL230gL-fCx1phRO\n",
            "To: /content/eval.txt\n",
            "100% 75.9M/75.9M [00:00<00:00, 135MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1MZQyevkz8w0XpecWUQu98v3BKaWIgho0\n",
        "!gdown 1dlPq2KgrTWJdopN96pi0LkU8BPaCld1N\n",
        "!gdown 1-7zEKtg2cMqo0s1SfL230gL-fCx1phRO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylEZwqTwlvhw"
      },
      "source": [
        "## Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AWd_52c66Hz7"
      },
      "outputs": [],
      "source": [
        "def create_dataset(filepath, label=None):\n",
        "    file = open(filepath, \"r\")\n",
        "    samples = file.readlines()\n",
        "    n_samples = len(samples)\n",
        "    n_features = 1752\n",
        "    x = np.zeros((n_samples, n_features))\n",
        "    for i, sample in enumerate(samples):\n",
        "        sample_features = sample.split()[1:]\n",
        "        x[i] = sample_features\n",
        "    n_classes = 2\n",
        "    if label == None:\n",
        "        y = None\n",
        "    else:\n",
        "        y = np.zeros((n_samples, 1))\n",
        "        y[:, 0] = label\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YzG3O-B8Gpok"
      },
      "outputs": [],
      "source": [
        "filepath = \"./benign.txt\"\n",
        "x_0, y_0 = create_dataset(filepath, 0)\n",
        "\n",
        "filepath = \"./maleware.txt\"\n",
        "x_1, y_1 = create_dataset(filepath, 1)\n",
        "\n",
        "filepath = \"./eval.txt\"\n",
        "x_eval, _ = create_dataset(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq-b9iHtGvCp",
        "outputId": "b3173026-690c-4b21-b417-2489b9f7547f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 1752)\n",
            "(20000, 1)\n",
            "(45000, 1752)\n",
            "(45000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_0.shape)\n",
        "print(y_0.shape)\n",
        "print(x_1.shape)\n",
        "print(y_1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_0, y_0 = shuffle(x_0, y_0, random_state=143)\n",
        "x_1, y_1 = shuffle(x_1, y_1, random_state=143)"
      ],
      "metadata": {
        "id": "sFER4LVoBZ-y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_val_class = 4000\n",
        "\n",
        "x_0_val = x_0[0:n_val_class]\n",
        "y_0_val = y_0[0:n_val_class]\n",
        "x_0_train = x_0[n_val_class:]\n",
        "y_0_train = y_0[n_val_class:]\n",
        "\n",
        "x_1_val = x_1[0:n_val_class]\n",
        "y_1_val = y_1[0:n_val_class]\n",
        "x_1_train = x_1[n_val_class:]\n",
        "y_1_train = y_1[n_val_class:]"
      ],
      "metadata": {
        "id": "IjQqYamlBz8k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VUeBqKOjmdFa"
      },
      "outputs": [],
      "source": [
        "zero_weight = x_0_train.shape[0] / (x_0_train.shape[0] + x_1_train.shape[0])\n",
        "one_weight = 1 - zero_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkRA5t_Mmxnc",
        "outputId": "af631151-a20f-4ce4-8369-5aad4834978a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2807017543859649\n",
            "0.7192982456140351\n"
          ]
        }
      ],
      "source": [
        "print(zero_weight)\n",
        "print(one_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2o3F1Wj2IZYf"
      },
      "outputs": [],
      "source": [
        "x_train = np.concatenate((x_0_train, x_1_train))\n",
        "y_train = np.concatenate((y_0_train, y_1_train))\n",
        "x_val = np.concatenate((x_0_val, x_1_val))\n",
        "y_val = np.concatenate((y_0_val, y_1_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = shuffle(x_train, y_train, random_state=143)\n",
        "x_val, y_val = shuffle(x_val, y_val, random_state=143)"
      ],
      "metadata": {
        "id": "dbuv6R4REZmA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSldHxWfl_ME",
        "outputId": "0300b7a1-08e2-4bf0-800f-8d6186832bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(57000, 1752)\n",
            "(57000, 1)\n",
            "(8000, 1752)\n",
            "(8000, 1)\n",
            "(10000, 1752)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_eval.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFulmt7yl61H"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kvyVd7KPd951"
      },
      "outputs": [],
      "source": [
        "x_all = np.concatenate((x_train, x_val, x_eval))\n",
        "data_mean = np.mean(x_all, axis=0)\n",
        "data_std = np.std(x_all, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hNCwl9tsgUVr"
      },
      "outputs": [],
      "source": [
        "x_train = (x_train - data_mean) / (data_std + 2.2e-16)\n",
        "x_val = (x_val - data_mean) / (data_std + 2.2e-16)\n",
        "x_eval = (x_eval - data_mean) / (data_std + 2.2e-16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxxEJNNJmNOQ"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9pIs2QjKOoDb"
      },
      "outputs": [],
      "source": [
        "def plot_results(title, train_metric, val_metric):\n",
        "    plt.title(title)\n",
        "    plt.plot(train_metric)\n",
        "    plt.plot(val_metric)\n",
        "    plt.ylabel(title)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['training', 'validation'], loc='upper right')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qRPkhXo-qBpB"
      },
      "outputs": [],
      "source": [
        "def create_plots(history):\n",
        "    plot_results(\n",
        "        title=\"Loss\",\n",
        "        train_metric=history.history['loss'],\n",
        "        val_metric=history.history['val_loss']\n",
        "    )\n",
        "    print()\n",
        "    plot_results(\n",
        "        title=\"Accuracy\",\n",
        "        train_metric=history.history['accuracy'],\n",
        "        val_metric=history.history['val_accuracy']\n",
        "    )\n",
        "    print()\n",
        "    plot_results(\n",
        "        title=\"Custom Metric\",\n",
        "        train_metric=history.history['cm'],\n",
        "        val_metric=history.history['val_cm']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tRavnSzNpnS_"
      },
      "outputs": [],
      "source": [
        "def create_wbce(zero_weight, one_weight):\n",
        "    def wbce(y_true, y_pred):\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * zero_weight + (1. - y_true) * one_weight\n",
        "        loss = weight_vector * bce\n",
        "        return K.mean(loss)\n",
        "    return wbce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZVGEWqbdqTyx"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def cm(y_true, y_pred):\n",
        "    p = precision_m(y_true, y_pred)\n",
        "    r = recall_m(y_true, y_pred)\n",
        "    return (1001 * p * r) / (p + (1000 * r) + K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_cm(n_preds_ones, n_exp_ones=5000):\n",
        "    fn = abs(n_exp_ones - n_preds_ones)\n",
        "    fp = fn\n",
        "    tp = n_preds_ones - fp\n",
        "    p = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    return (1001 * p * r) / (p + (1000 * r))"
      ],
      "metadata": {
        "id": "qvhQiGeJ8xmq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tPr6ygtXOjhX"
      },
      "outputs": [],
      "source": [
        "def compile_and_fit(model, epochs=10, batch_size=32):\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath='./checkpoints/{epoch:05d}.hdf5',\n",
        "        monitor='val_cm',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(),\n",
        "        loss=create_wbce(zero_weight, one_weight),\n",
        "        metrics=[cm, 'accuracy']\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        validation_data=[x_val, y_val],\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint]\n",
        "    )\n",
        "\n",
        "    custom_objects = {\n",
        "        'wbce': create_wbce(zero_weight, one_weight),\n",
        "        'cm': cm\n",
        "    }\n",
        "\n",
        "    list_of_files = glob.glob('./checkpoints/*')\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    best_epoch = latest_file[14:19]\n",
        "    \n",
        "    print('\\nBest Epoch: ' + str(best_epoch))\n",
        "\n",
        "    best_model = load_model(latest_file, custom_objects=custom_objects)\n",
        "\n",
        "    for f in list_of_files:\n",
        "        os.remove(f)\n",
        "    \n",
        "    return best_model, model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Dx_MFf0MpV8p"
      },
      "outputs": [],
      "source": [
        "def do_the_all(model, epochs=10, batch_size=32, do_plots=False):\n",
        "    best_model, last_model, history = compile_and_fit(model, epochs, batch_size)\n",
        "    best_model.save('./checkpoints/best_model.hdf5')\n",
        "    \n",
        "    if do_plots:\n",
        "        print()\n",
        "        create_plots(history)\n",
        "    \n",
        "    print(\"\\nTrain Result\")\n",
        "    best_model.evaluate(\n",
        "        x=x_train,\n",
        "        y=y_train\n",
        "    )\n",
        "\n",
        "    print(\"\\nVal Result\")\n",
        "    best_model.evaluate(\n",
        "        x=x_val,\n",
        "        y=y_val\n",
        "    )\n",
        "\n",
        "    print(\"\\nPredictions\")\n",
        "    preds = best_model.predict(x_eval)\n",
        "    new_preds = np.int64(preds > 0.5).squeeze()\n",
        "\n",
        "    n_viruses = new_preds.sum()\n",
        "    print(f\"\\nNumber of Viruses: {n_viruses}\")\n",
        "    print(f\"Estimate Score: {estimate_cm(n_viruses, new_preds.shape[0]/2)}\\n\")\n",
        "\n",
        "    new_preds = [str(x) for x in new_preds]\n",
        "    content = \",\".join(new_preds)\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_p-5PUkwkft"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n-HyJJcRMXcn",
        "outputId": "e98a7714-65c0-40fb-8a65-53f0b91ed4cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1752)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               224384    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 274,049\n",
            "Trainable params: 274,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1782/1782 [==============================] - 14s 7ms/step - loss: 0.3881 - cm: 0.9598 - accuracy: 0.9194 - val_loss: 0.1563 - val_cm: 0.9266 - val_accuracy: 0.9289\n",
            "Epoch 2/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1371 - cm: 0.9643 - accuracy: 0.9310 - val_loss: 0.1480 - val_cm: 0.9281 - val_accuracy: 0.9281\n",
            "Epoch 3/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1331 - cm: 0.9650 - accuracy: 0.9317 - val_loss: 0.1574 - val_cm: 0.9114 - val_accuracy: 0.9339\n",
            "Epoch 4/50\n",
            "1782/1782 [==============================] - 12s 7ms/step - loss: 0.1310 - cm: 0.9661 - accuracy: 0.9318 - val_loss: 0.1482 - val_cm: 0.9318 - val_accuracy: 0.9289\n",
            "Epoch 5/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1313 - cm: 0.9664 - accuracy: 0.9321 - val_loss: 0.1600 - val_cm: 0.9042 - val_accuracy: 0.9304\n",
            "Epoch 6/50\n",
            "1782/1782 [==============================] - 10s 6ms/step - loss: 0.1305 - cm: 0.9660 - accuracy: 0.9313 - val_loss: 0.1497 - val_cm: 0.9293 - val_accuracy: 0.9367\n",
            "Epoch 7/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1323 - cm: 0.9660 - accuracy: 0.9314 - val_loss: 0.1532 - val_cm: 0.9242 - val_accuracy: 0.9304\n",
            "Epoch 8/50\n",
            "1782/1782 [==============================] - 13s 7ms/step - loss: 0.1254 - cm: 0.9676 - accuracy: 0.9326 - val_loss: 0.1492 - val_cm: 0.9136 - val_accuracy: 0.9329\n",
            "Epoch 9/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1328 - cm: 0.9665 - accuracy: 0.9323 - val_loss: 0.1454 - val_cm: 0.9292 - val_accuracy: 0.9289\n",
            "Epoch 10/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1287 - cm: 0.9662 - accuracy: 0.9320 - val_loss: 0.1412 - val_cm: 0.9222 - val_accuracy: 0.9344\n",
            "Epoch 11/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1286 - cm: 0.9666 - accuracy: 0.9306 - val_loss: 0.1870 - val_cm: 0.9330 - val_accuracy: 0.9309\n",
            "Epoch 12/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1318 - cm: 0.9666 - accuracy: 0.9308 - val_loss: 0.1578 - val_cm: 0.9103 - val_accuracy: 0.9314\n",
            "Epoch 13/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1279 - cm: 0.9662 - accuracy: 0.9311 - val_loss: 0.1400 - val_cm: 0.9238 - val_accuracy: 0.9245\n",
            "Epoch 14/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1288 - cm: 0.9666 - accuracy: 0.9311 - val_loss: 0.1811 - val_cm: 0.9279 - val_accuracy: 0.9312\n",
            "Epoch 15/50\n",
            "1782/1782 [==============================] - 12s 7ms/step - loss: 0.1279 - cm: 0.9666 - accuracy: 0.9308 - val_loss: 0.1397 - val_cm: 0.9269 - val_accuracy: 0.9271\n",
            "Epoch 16/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1268 - cm: 0.9670 - accuracy: 0.9321 - val_loss: 0.1361 - val_cm: 0.9321 - val_accuracy: 0.9295\n",
            "Epoch 17/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1296 - cm: 0.9669 - accuracy: 0.9322 - val_loss: 0.1387 - val_cm: 0.9328 - val_accuracy: 0.9298\n",
            "Epoch 18/50\n",
            "1782/1782 [==============================] - 10s 6ms/step - loss: 0.1261 - cm: 0.9669 - accuracy: 0.9312 - val_loss: 0.2975 - val_cm: 0.9054 - val_accuracy: 0.9212\n",
            "Epoch 19/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1325 - cm: 0.9662 - accuracy: 0.9299 - val_loss: 0.1425 - val_cm: 0.9240 - val_accuracy: 0.9337\n",
            "Epoch 20/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1226 - cm: 0.9669 - accuracy: 0.9316 - val_loss: 0.1510 - val_cm: 0.9109 - val_accuracy: 0.9308\n",
            "Epoch 21/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1272 - cm: 0.9667 - accuracy: 0.9315 - val_loss: 0.1471 - val_cm: 0.9247 - val_accuracy: 0.9330\n",
            "Epoch 22/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1316 - cm: 0.9668 - accuracy: 0.9306 - val_loss: 0.1489 - val_cm: 0.9183 - val_accuracy: 0.9333\n",
            "Epoch 23/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1256 - cm: 0.9663 - accuracy: 0.9308 - val_loss: 0.1444 - val_cm: 0.9216 - val_accuracy: 0.9339\n",
            "Epoch 24/50\n",
            "1782/1782 [==============================] - 10s 6ms/step - loss: 0.1282 - cm: 0.9667 - accuracy: 0.9310 - val_loss: 0.1383 - val_cm: 0.9290 - val_accuracy: 0.9312\n",
            "Epoch 25/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1275 - cm: 0.9663 - accuracy: 0.9309 - val_loss: 0.1501 - val_cm: 0.9147 - val_accuracy: 0.9290\n",
            "Epoch 26/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1258 - cm: 0.9670 - accuracy: 0.9311 - val_loss: 0.1430 - val_cm: 0.9265 - val_accuracy: 0.9311\n",
            "Epoch 27/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1276 - cm: 0.9669 - accuracy: 0.9307 - val_loss: 0.1489 - val_cm: 0.9221 - val_accuracy: 0.9237\n",
            "Epoch 28/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1306 - cm: 0.9664 - accuracy: 0.9301 - val_loss: 0.1381 - val_cm: 0.9289 - val_accuracy: 0.9324\n",
            "Epoch 29/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1266 - cm: 0.9668 - accuracy: 0.9305 - val_loss: 0.1506 - val_cm: 0.9123 - val_accuracy: 0.9175\n",
            "Epoch 30/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1300 - cm: 0.9668 - accuracy: 0.9305 - val_loss: 0.1558 - val_cm: 0.9344 - val_accuracy: 0.9275\n",
            "Epoch 31/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1283 - cm: 0.9667 - accuracy: 0.9301 - val_loss: 0.1393 - val_cm: 0.9275 - val_accuracy: 0.9250\n",
            "Epoch 32/50\n",
            "1782/1782 [==============================] - 13s 7ms/step - loss: 0.1274 - cm: 0.9661 - accuracy: 0.9307 - val_loss: 0.1529 - val_cm: 0.9167 - val_accuracy: 0.9315\n",
            "Epoch 33/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1272 - cm: 0.9664 - accuracy: 0.9311 - val_loss: 0.1512 - val_cm: 0.9254 - val_accuracy: 0.9305\n",
            "Epoch 34/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1246 - cm: 0.9670 - accuracy: 0.9310 - val_loss: 0.1387 - val_cm: 0.9294 - val_accuracy: 0.9299\n",
            "Epoch 35/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1264 - cm: 0.9673 - accuracy: 0.9312 - val_loss: 0.1521 - val_cm: 0.9204 - val_accuracy: 0.9325\n",
            "Epoch 36/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1310 - cm: 0.9665 - accuracy: 0.9303 - val_loss: 0.1385 - val_cm: 0.9253 - val_accuracy: 0.9290\n",
            "Epoch 37/50\n",
            "1782/1782 [==============================] - 10s 6ms/step - loss: 0.1258 - cm: 0.9659 - accuracy: 0.9296 - val_loss: 0.1399 - val_cm: 0.9226 - val_accuracy: 0.9268\n",
            "Epoch 38/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1267 - cm: 0.9667 - accuracy: 0.9307 - val_loss: 0.1451 - val_cm: 0.9216 - val_accuracy: 0.9316\n",
            "Epoch 39/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1294 - cm: 0.9666 - accuracy: 0.9312 - val_loss: 0.1453 - val_cm: 0.9169 - val_accuracy: 0.9276\n",
            "Epoch 40/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1233 - cm: 0.9662 - accuracy: 0.9297 - val_loss: 0.1336 - val_cm: 0.9316 - val_accuracy: 0.9294\n",
            "Epoch 41/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1273 - cm: 0.9670 - accuracy: 0.9310 - val_loss: 0.1373 - val_cm: 0.9331 - val_accuracy: 0.9220\n",
            "Epoch 42/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1288 - cm: 0.9661 - accuracy: 0.9302 - val_loss: 0.1374 - val_cm: 0.9219 - val_accuracy: 0.9342\n",
            "Epoch 43/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1238 - cm: 0.9656 - accuracy: 0.9300 - val_loss: 0.1757 - val_cm: 0.9035 - val_accuracy: 0.9289\n",
            "Epoch 44/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1285 - cm: 0.9668 - accuracy: 0.9306 - val_loss: 0.1416 - val_cm: 0.9193 - val_accuracy: 0.9269\n",
            "Epoch 45/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1248 - cm: 0.9657 - accuracy: 0.9296 - val_loss: 0.1649 - val_cm: 0.9229 - val_accuracy: 0.9314\n",
            "Epoch 46/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1258 - cm: 0.9666 - accuracy: 0.9311 - val_loss: 0.1361 - val_cm: 0.9275 - val_accuracy: 0.9327\n",
            "Epoch 47/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1241 - cm: 0.9670 - accuracy: 0.9309 - val_loss: 0.1497 - val_cm: 0.9301 - val_accuracy: 0.9234\n",
            "Epoch 48/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1288 - cm: 0.9660 - accuracy: 0.9297 - val_loss: 0.1622 - val_cm: 0.9260 - val_accuracy: 0.9312\n",
            "Epoch 49/50\n",
            "1782/1782 [==============================] - 10s 6ms/step - loss: 0.1235 - cm: 0.9673 - accuracy: 0.9315 - val_loss: 0.1443 - val_cm: 0.9239 - val_accuracy: 0.9308\n",
            "Epoch 50/50\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.1261 - cm: 0.9670 - accuracy: 0.9310 - val_loss: 0.1437 - val_cm: 0.9203 - val_accuracy: 0.9311\n",
            "\n",
            "Best Epoch: 00030\n",
            "\n",
            "Train Result\n",
            "1782/1782 [==============================] - 5s 3ms/step - loss: 0.1376 - cm: 0.9726 - accuracy: 0.9245\n",
            "\n",
            "Val Result\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.1558 - cm: 0.9344 - accuracy: 0.9275\n",
            "\n",
            "Predictions\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "\n",
            "Number of Viruses: 4933\n",
            "Estimate Score: 0.9864180012162984\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1,1,0,1,1,0,0,1,1,1,1,1,1,0,0,1,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,1,1,1,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,1,1,0,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,1,0,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,0,1,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,0,0,0,1,1,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,0,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,0,1,0,1,0,0,0,0,0,0,0,0,1,1,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,1,1,1,0,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,1,0,1,1,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,0,0,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,0,0,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,1,0,1,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,1,1,0,1,0,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,1,1,0,0,0,0,0,1,0,0,0,0,1,1,0,1,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,0,0,1,0,1,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1,1,1,0,0,1,0,1,0,1,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,1,0,1,0,0,1,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,0,1,1,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1,0,0,0,1,0,1,0,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,1,1,0,1,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,0,1,0,0,0,1,0,1,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,1,0,1,0,1,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,1,0,1,1,0,0,1,1,0,1,1,0,1,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,1,0,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,1,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,0,1,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,1,1,0,0,1,1,0,0,0,0,0,1,0,1,0,0,0,1,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,1,0,1,0,1,1,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,1,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,1,0,0,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,0,0,1,1,1,0,1,0,1,1,0,1,1,1,0,1,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1,0,0,1,1,1,0,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,1,1,1,1,1,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,1,0,0,1,1,1,1,0,0,1,0,1,1,1,0,0,1,0,1,0,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,0,0,1,0,1,1,1,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,1,1,1,0,0,1,1,0,1,0,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,1,1,0,1,0,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,0,0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,1,0,1,0,1,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,0,1,0,0,0,0,1,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,1,0,1,1,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,1,1,1,0,1,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,1,0,1,0,0,1,1,0,0,0,0,1,1,1,0,1,1,1,1,0,1,0,0,1,0,1,0,1,1,0,0,1,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,1,0,0,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,0,1,0,1,0,1,0,1,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,0,1,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0,1,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,1,1,0,1,0,0,1,1,1,0,1,1,1,1,0,0,1,0,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,1,1,1,0,1,0,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,1,0,0,0,1,0,0,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,1,1,1,0,1,0,1,0,0,0,1,0,1,1,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,1,1,1,0,1,0,1,1,0,1,0,1,0,1,1,0,1,0,1,1,1,1,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,0,1,1,0,1,1,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,0,1,1,0,1,1,1,1,0,0,0,1,0,0,1,0,0,1,1,0,0,0,1,1,0,0,1,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,1,1,1,1,1,0,1,1,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,0,1,1,0,1,0,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,1,1,1,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,0,0,0,0,1,1,0,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,1,0,0,1,1,1,0,1,1,0,0,0,1,1,1,1,0,0,0,0,1,1,1,0,1,1,1,1,0,1,1,0,1,0,1,0,1,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,0,1,0,0,0,1,0,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,0,0,1,1,1,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,1,0,1,0,0,0,1,0,0,0,0,1,1,1,1,0,0,1,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,1,0,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,0,1,1,0,1,0,0,0,1,1,0,1,1,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,1,1,1,0,1,0,0,0,1,0,1,1,1,1,0,1,1,0,1,1,1,0,0,0,0,1,1,1,1,1,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,1,1,1,1,1,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,0,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,1,1,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,1,0,0,0,0,0,1,0,1,0,0,1,1,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,1,0,0,1,0,1,0,0,1,0,1,0,1,0,1,0,0,1,1,1,0,1,0,0,0,0,0,1,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,0,1,0,0,0,1,1,1,1,1,0,0,0,0,1,1,0,1,0,1,0,1,1,0,0,1,1,1,0,1,1,1,1,1,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,0,0,0,1,1,1,0,0,0,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,0,1,1,1,0,0,1,1,0,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,0,1,1,0,1,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,0,1,0,1,0,0,1,0,0,1,1,1,1,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,1,1,0,1,1,1,1,1,1,0,0,1,0,0,1,1,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,1,1,0,1,1,0,0,0,1,0,0,1,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,1,1,0,1,0,1,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,1,0,1,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,1,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,1,1,1,1,1,0,1,0,1,0,0,1,0,1,1,0,1,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,1,0,1,1,1,1,0,0,1,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,1,0,0,1,0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,1,0,0,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,1,1,1,0,0,1,1,0,1,0,0,1,1,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,0,1,1,0,1,1,1,0,0,0,0,1,1,0,1,0,1,1,1,0,0,1,0,0,0,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,1,1,0,1,0,1,1,1,0,1,0,1,0,0,1,0,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,0,1,0,1,0,1,1,0,1,1,0,0,0,0,1,0,1,1,1,1,0,1,1,0,0,1,1,0,1,1,0,1,1,0,1,1,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,0,1,1,1,0,0,0,0,1,1,1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,1,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,0,1,1,1,0,1,1,0,1,0,0,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,0,1,0,1,0,1,0,0,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,0,0,1,1,1,0,1,1,0,0,0,1,0,1,0,0,1,1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,0,1,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,1,0,0,0,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,1,1,1,1,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,1,1,0,1,0,1,0,0,1,0,1,1,1,0,1,0,0,1,1,0,0,0,0,0,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,1,1,1,1,0,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,1,1,1,0,1,1,0,1,1,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,0,1,1,1,1,0,1,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,0,1,1,1,0,1,0,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,0,0,1,0,1,0,1,1,0,0,1,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,1,1,0,1,1,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,1,1,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,0,1,1,0,1,1,0,1,0,1,0,1,1,0,0,1,1,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,1,1,1,1,1,1,0,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,1,1,0,1,0,1,1,0,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,0,1,1,0,0,1,0,1,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,1,0,0,0,1,1,0,0,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,1,0,0,0,1,0,0,0,0,1,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,1,1,0,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,0,0,0,0,1,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,0,0,1,1,0,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,1,0,0,1,1,1,0,1,0,0,0,1,0,1,0,1,1,0,1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,0,0,1,0,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,1,0,0,1,1,1,0,1,1,1,1,1,1,0,1,0,1,0,0,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,0,1,1,1,1,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,1,1,0,0,1,1,1,0,0,1,1,0,0,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,1,0,0,1,1,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,0,1,0,1,0,0,0,0,0,0,1,0,1,1,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,1,1,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,1,1,0,0,1,0,0,1,1,0,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,0,1,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,1,1,0,0,0,1,1,1,0,0,0,0,0,1,0,0,1,0,1,0,1,1,1,1,0,0,1,0,1,0,1,1,0,1,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,1,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,1,0,0,1,1,0,1,0,1,1,0,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,0,0,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,1,1,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,0,1,1,1,0,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,0,0,1,0,1,1,1,0,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,1,0,0,0,0,1,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,1,1,1,0,1,0,0,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,0,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,1,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,1,0,1,1,1,0,0,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,1,1,1,1,0,0,1,1,0,1,0,0,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,1,0,0,0,1,0,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,1,0,1,1,0,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,0,1,0,0,1,1,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,1,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,0,1,1,1,0,1,1,0,0,1,1,0,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,0,0,1,1,0,0,0,0,0,1,0,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,1,0,1,1,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,0,1,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,0,0,1,1,1,0,1,0,1,1,0,0,1,0,0,1,0,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,1,1,1,1,1,1,1,0,0,1,1,0,1,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,0,1,0,1,0,0,0,0,1,0,1,1,1,1,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,0,1,1,0,0,1,1,0,1,1,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,1,0,1,1,1,0,0,0,1,1,1,1,0,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,1,0,1,1,0,1,1,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,1,1,0,1,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,0,1,1,0,1,1,0,1,0,0,1,1,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,1,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,1,1,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,1,1,0,1,1,0,0,0,1,1,0,1,0,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,1,0,1,0,1,0,0,0,0,1,1,0,0,1,1,1,0,0,0,0,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,0,0,1,1,1,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,1,0,0,1,0,0,0,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,1,0,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,1,1,0,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,0,0,1,1,1,1,0,0,1,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,0,1,0,1,0,0,0,0,1,0,0,0,1,1,1,1,1,1,0,1,1,0,0,1,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,0,1,1,0,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,1,1,0,1,1,1,0,0,0,0,0,1,1,0,1,0,1,1,0,0,1,0,1,0,0,1,1,0,1,1,0,1,1,1,1,0,1,0,0,1,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,0,0,0,0,1,1,1,1,0,1,0,1,1,0,0,1,0,0,0,0,0,1,0,1,1,0,1,0,0,1,0,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,0,1,0,1,1,0,0,0,0,0,1,0,1,1,1,0,1,1,0,0,1,1,0,0,0,0,1,0,0,1,0,1,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,0,0,0,1,0,0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def model_1(n_features=1752):\n",
        "    l1 = 0.0005\n",
        "    l2 = 0.005\n",
        "\n",
        "    x_input = layers.Input(shape=(n_features))\n",
        "    x = layers.Dense(128, activation='elu', kernel_regularizer=L1L2(l1, l2))(x_input)\n",
        "    x = layers.Dense(128, activation='elu', kernel_regularizer=L1L2(l1, l2))(x)\n",
        "    x = layers.Dense(128, activation='elu', kernel_regularizer=L1L2(l1, l2))(x)\n",
        "    x = layers.Dense(128, activation='elu', kernel_regularizer=L1L2(l1, l2))(x)\n",
        "    x_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(x_input, x_output, name=\"model_1\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "do_the_all(model=model_1(), epochs=50, batch_size=32, do_plots=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1GGOrAsrtKE5",
        "outputId": "d4971fd2-3023-4128-dc9e-bab15fd61d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 1752)]            0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 512)               897536    \n",
            "                                                                 \n",
            " layer_normalization (LayerN  (None, 512)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " layer_normalization_1 (Laye  (None, 512)              1024      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " layer_normalization_2 (Laye  (None, 128)              256       \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,228,289\n",
            "Trainable params: 1,228,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1782/1782 [==============================] - 37s 20ms/step - loss: 0.0871 - cm: 0.9627 - accuracy: 0.9221 - val_loss: 0.0891 - val_cm: 0.9276 - val_accuracy: 0.9298\n",
            "Epoch 2/50\n",
            "1782/1782 [==============================] - 36s 20ms/step - loss: 0.0613 - cm: 0.9734 - accuracy: 0.9467 - val_loss: 0.0746 - val_cm: 0.9421 - val_accuracy: 0.9449\n",
            "Epoch 3/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0526 - cm: 0.9766 - accuracy: 0.9535 - val_loss: 0.0784 - val_cm: 0.9463 - val_accuracy: 0.9484\n",
            "Epoch 4/50\n",
            "1782/1782 [==============================] - 35s 19ms/step - loss: 0.0468 - cm: 0.9805 - accuracy: 0.9597 - val_loss: 0.0756 - val_cm: 0.9485 - val_accuracy: 0.9463\n",
            "Epoch 5/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0428 - cm: 0.9822 - accuracy: 0.9625 - val_loss: 0.0785 - val_cm: 0.9430 - val_accuracy: 0.9514\n",
            "Epoch 6/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0383 - cm: 0.9839 - accuracy: 0.9664 - val_loss: 0.0875 - val_cm: 0.9417 - val_accuracy: 0.9504\n",
            "Epoch 7/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0355 - cm: 0.9850 - accuracy: 0.9690 - val_loss: 0.0870 - val_cm: 0.9377 - val_accuracy: 0.9526\n",
            "Epoch 8/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0333 - cm: 0.9857 - accuracy: 0.9712 - val_loss: 0.0909 - val_cm: 0.9324 - val_accuracy: 0.9495\n",
            "Epoch 9/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0316 - cm: 0.9863 - accuracy: 0.9726 - val_loss: 0.0936 - val_cm: 0.9339 - val_accuracy: 0.9505\n",
            "Epoch 10/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0293 - cm: 0.9875 - accuracy: 0.9746 - val_loss: 0.0930 - val_cm: 0.9456 - val_accuracy: 0.9506\n",
            "Epoch 11/50\n",
            "1782/1782 [==============================] - 37s 21ms/step - loss: 0.0279 - cm: 0.9883 - accuracy: 0.9754 - val_loss: 0.1044 - val_cm: 0.9363 - val_accuracy: 0.9499\n",
            "Epoch 12/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0268 - cm: 0.9885 - accuracy: 0.9768 - val_loss: 0.1045 - val_cm: 0.9321 - val_accuracy: 0.9505\n",
            "Epoch 13/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0259 - cm: 0.9884 - accuracy: 0.9771 - val_loss: 0.1048 - val_cm: 0.9333 - val_accuracy: 0.9505\n",
            "Epoch 14/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0247 - cm: 0.9885 - accuracy: 0.9773 - val_loss: 0.1084 - val_cm: 0.9399 - val_accuracy: 0.9535\n",
            "Epoch 15/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0241 - cm: 0.9896 - accuracy: 0.9788 - val_loss: 0.0935 - val_cm: 0.9411 - val_accuracy: 0.9526\n",
            "Epoch 16/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0229 - cm: 0.9903 - accuracy: 0.9800 - val_loss: 0.1104 - val_cm: 0.9389 - val_accuracy: 0.9516\n",
            "Epoch 17/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0223 - cm: 0.9898 - accuracy: 0.9803 - val_loss: 0.1233 - val_cm: 0.9279 - val_accuracy: 0.9510\n",
            "Epoch 18/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0220 - cm: 0.9901 - accuracy: 0.9804 - val_loss: 0.0985 - val_cm: 0.9427 - val_accuracy: 0.9514\n",
            "Epoch 19/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0213 - cm: 0.9903 - accuracy: 0.9810 - val_loss: 0.1218 - val_cm: 0.9338 - val_accuracy: 0.9514\n",
            "Epoch 20/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0207 - cm: 0.9904 - accuracy: 0.9815 - val_loss: 0.1210 - val_cm: 0.9404 - val_accuracy: 0.9539\n",
            "Epoch 21/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0201 - cm: 0.9909 - accuracy: 0.9821 - val_loss: 0.1188 - val_cm: 0.9360 - val_accuracy: 0.9541\n",
            "Epoch 22/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0198 - cm: 0.9913 - accuracy: 0.9820 - val_loss: 0.1085 - val_cm: 0.9426 - val_accuracy: 0.9555\n",
            "Epoch 23/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0192 - cm: 0.9913 - accuracy: 0.9823 - val_loss: 0.1380 - val_cm: 0.9327 - val_accuracy: 0.9507\n",
            "Epoch 24/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0194 - cm: 0.9910 - accuracy: 0.9826 - val_loss: 0.1154 - val_cm: 0.9389 - val_accuracy: 0.9548\n",
            "Epoch 25/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0185 - cm: 0.9915 - accuracy: 0.9832 - val_loss: 0.1317 - val_cm: 0.9277 - val_accuracy: 0.9516\n",
            "Epoch 26/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0186 - cm: 0.9915 - accuracy: 0.9838 - val_loss: 0.1216 - val_cm: 0.9438 - val_accuracy: 0.9506\n",
            "Epoch 27/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0182 - cm: 0.9919 - accuracy: 0.9837 - val_loss: 0.1243 - val_cm: 0.9340 - val_accuracy: 0.9520\n",
            "Epoch 28/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0177 - cm: 0.9914 - accuracy: 0.9841 - val_loss: 0.1267 - val_cm: 0.9366 - val_accuracy: 0.9542\n",
            "Epoch 29/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0175 - cm: 0.9918 - accuracy: 0.9846 - val_loss: 0.1186 - val_cm: 0.9383 - val_accuracy: 0.9541\n",
            "Epoch 30/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0168 - cm: 0.9925 - accuracy: 0.9848 - val_loss: 0.1384 - val_cm: 0.9298 - val_accuracy: 0.9539\n",
            "Epoch 31/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0167 - cm: 0.9921 - accuracy: 0.9846 - val_loss: 0.1389 - val_cm: 0.9344 - val_accuracy: 0.9524\n",
            "Epoch 32/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0167 - cm: 0.9925 - accuracy: 0.9853 - val_loss: 0.1360 - val_cm: 0.9277 - val_accuracy: 0.9510\n",
            "Epoch 33/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0164 - cm: 0.9927 - accuracy: 0.9853 - val_loss: 0.1300 - val_cm: 0.9367 - val_accuracy: 0.9535\n",
            "Epoch 34/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0163 - cm: 0.9926 - accuracy: 0.9858 - val_loss: 0.1337 - val_cm: 0.9364 - val_accuracy: 0.9546\n",
            "Epoch 35/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0158 - cm: 0.9928 - accuracy: 0.9858 - val_loss: 0.1274 - val_cm: 0.9369 - val_accuracy: 0.9559\n",
            "Epoch 36/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0154 - cm: 0.9929 - accuracy: 0.9859 - val_loss: 0.1424 - val_cm: 0.9327 - val_accuracy: 0.9531\n",
            "Epoch 37/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0150 - cm: 0.9928 - accuracy: 0.9867 - val_loss: 0.1277 - val_cm: 0.9390 - val_accuracy: 0.9544\n",
            "Epoch 38/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0154 - cm: 0.9929 - accuracy: 0.9865 - val_loss: 0.1340 - val_cm: 0.9357 - val_accuracy: 0.9532\n",
            "Epoch 39/50\n",
            "1782/1782 [==============================] - 40s 22ms/step - loss: 0.0150 - cm: 0.9933 - accuracy: 0.9866 - val_loss: 0.1414 - val_cm: 0.9282 - val_accuracy: 0.9510\n",
            "Epoch 40/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0149 - cm: 0.9929 - accuracy: 0.9866 - val_loss: 0.1386 - val_cm: 0.9361 - val_accuracy: 0.9535\n",
            "Epoch 41/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0147 - cm: 0.9934 - accuracy: 0.9869 - val_loss: 0.1377 - val_cm: 0.9342 - val_accuracy: 0.9545\n",
            "Epoch 42/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0144 - cm: 0.9930 - accuracy: 0.9869 - val_loss: 0.1337 - val_cm: 0.9367 - val_accuracy: 0.9545\n",
            "Epoch 43/50\n",
            "1782/1782 [==============================] - 37s 21ms/step - loss: 0.0149 - cm: 0.9927 - accuracy: 0.9861 - val_loss: 0.1348 - val_cm: 0.9302 - val_accuracy: 0.9501\n",
            "Epoch 44/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0143 - cm: 0.9933 - accuracy: 0.9874 - val_loss: 0.1423 - val_cm: 0.9335 - val_accuracy: 0.9546\n",
            "Epoch 45/50\n",
            "1782/1782 [==============================] - 35s 19ms/step - loss: 0.0144 - cm: 0.9934 - accuracy: 0.9871 - val_loss: 0.1385 - val_cm: 0.9317 - val_accuracy: 0.9532\n",
            "Epoch 46/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0136 - cm: 0.9933 - accuracy: 0.9877 - val_loss: 0.1544 - val_cm: 0.9327 - val_accuracy: 0.9530\n",
            "Epoch 47/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0144 - cm: 0.9932 - accuracy: 0.9868 - val_loss: 0.1334 - val_cm: 0.9352 - val_accuracy: 0.9557\n",
            "Epoch 48/50\n",
            "1782/1782 [==============================] - 33s 18ms/step - loss: 0.0135 - cm: 0.9934 - accuracy: 0.9872 - val_loss: 0.1421 - val_cm: 0.9353 - val_accuracy: 0.9540\n",
            "Epoch 49/50\n",
            "1782/1782 [==============================] - 34s 19ms/step - loss: 0.0137 - cm: 0.9934 - accuracy: 0.9876 - val_loss: 0.1350 - val_cm: 0.9354 - val_accuracy: 0.9529\n",
            "Epoch 50/50\n",
            "1782/1782 [==============================] - 33s 19ms/step - loss: 0.0137 - cm: 0.9936 - accuracy: 0.9876 - val_loss: 0.1288 - val_cm: 0.9359 - val_accuracy: 0.9554\n",
            "\n",
            "Best Epoch: 00004\n",
            "\n",
            "Train Result\n",
            "1782/1782 [==============================] - 11s 6ms/step - loss: 0.0419 - cm: 0.9877 - accuracy: 0.9560\n",
            "\n",
            "Val Result\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0756 - cm: 0.9485 - accuracy: 0.9463\n",
            "\n",
            "Predictions\n",
            "313/313 [==============================] - 2s 7ms/step\n",
            "\n",
            "Number of Viruses: 5026\n",
            "Estimate Score: 0.9948269001193792\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1,1,0,1,1,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,0,0,1,0,0,0,1,1,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,1,1,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,0,1,1,0,0,1,0,1,1,0,0,1,0,0,1,1,0,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,0,1,0,1,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,0,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,0,1,0,0,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,1,1,1,0,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,1,0,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,0,1,0,0,0,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,0,1,0,1,0,0,0,1,0,1,1,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,0,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,0,0,1,0,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,1,0,1,0,0,1,0,1,1,0,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,1,1,1,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,0,0,0,1,1,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,0,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,0,0,0,0,1,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,0,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,1,0,0,1,1,1,1,0,1,1,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,1,0,1,0,0,1,1,1,0,1,1,0,0,0,1,0,0,1,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,1,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,1,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,1,1,0,0,1,1,0,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,1,0,1,0,1,1,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,0,0,1,1,1,0,0,0,1,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,1,1,1,1,0,0,1,1,0,0,1,1,0,1,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,0,1,0,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0,1,1,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,1,1,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,0,1,0,1,0,0,0,1,0,0,0,1,0,0,1,1,1,0,1,0,0,0,0,1,1,1,0,0,1,1,0,1,0,1,1,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,1,0,1,0,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,1,1,0,1,0,1,1,1,1,0,1,1,0,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,1,0,0,1,0,1,0,1,1,1,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,0,1,1,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,1,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,1,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,1,0,1,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,1,1,0,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,0,1,0,1,0,1,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,0,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,0,1,1,0,1,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,1,1,0,1,0,1,1,1,1,0,0,1,1,1,0,0,1,0,0,0,1,1,0,0,0,1,0,0,1,1,0,0,1,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,0,1,1,0,0,0,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,1,0,0,1,0,0,1,1,0,1,0,0,0,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,1,1,0,1,1,0,0,1,0,1,1,1,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,0,1,0,1,0,0,1,1,0,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,0,0,1,1,1,1,0,1,1,1,1,0,1,0,1,1,0,1,1,1,1,0,0,0,1,0,0,1,0,0,1,1,0,0,0,1,1,0,0,1,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,0,1,0,1,0,1,0,0,0,1,1,1,1,0,1,1,0,1,0,0,1,0,0,1,0,0,1,1,1,1,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,1,1,1,0,1,0,0,1,1,1,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,0,1,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,0,0,1,1,0,0,1,1,0,1,1,1,0,1,0,0,1,1,0,1,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,0,1,1,0,1,1,0,0,0,1,1,0,0,1,0,0,0,1,1,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,1,0,0,1,1,0,1,1,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,0,0,1,0,1,1,0,0,0,1,1,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,1,0,1,0,0,0,1,0,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,0,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,0,0,0,0,1,0,1,1,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,0,1,1,1,0,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,0,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,1,0,1,1,1,0,1,1,1,0,0,1,1,1,0,1,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,1,0,1,1,0,0,1,1,1,1,1,1,1,0,1,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,0,0,1,0,1,0,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,1,1,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,1,0,1,0,1,1,1,1,0,1,1,1,0,1,0,0,0,0,1,1,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,1,0,1,1,1,1,0,0,1,0,1,0,0,0,1,1,1,1,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,1,0,1,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,0,0,1,0,1,1,0,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,0,0,0,1,1,0,0,0,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,1,1,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,0,1,1,1,1,1,1,1,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,0,1,0,1,0,0,1,0,0,1,1,0,1,1,0,1,1,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,0,0,1,0,0,1,1,0,1,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,0,1,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,0,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,0,1,1,1,1,0,1,0,1,0,0,1,0,1,1,0,1,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,1,0,0,0,1,1,1,1,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,0,0,1,1,0,1,1,1,0,0,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,0,1,0,1,0,1,0,0,1,1,0,0,1,0,1,0,1,1,0,1,0,1,1,1,0,1,1,0,1,1,0,1,0,0,1,1,0,0,1,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,0,1,1,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,1,1,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,1,1,1,0,1,1,0,1,0,0,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,0,0,1,1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,1,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,1,1,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,1,1,1,1,0,1,0,0,1,1,0,1,1,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,1,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,0,1,1,1,1,0,1,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,0,1,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,0,1,0,1,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,1,1,0,0,1,1,0,0,0,1,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,0,0,1,0,1,0,1,1,0,0,0,1,0,1,0,0,0,0,0,1,0,1,0,0,0,1,1,1,1,0,1,0,0,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,0,1,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,0,0,0,1,1,1,0,0,0,0,0,1,0,1,0,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,1,0,1,1,0,1,1,0,1,0,0,1,0,1,1,0,1,1,0,0,0,1,0,1,1,0,0,1,1,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,1,1,1,0,1,1,0,0,1,0,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,1,1,0,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,0,0,1,0,0,0,1,0,1,0,1,1,1,0,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,0,1,1,1,0,0,0,1,1,0,1,0,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,0,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,1,0,1,1,0,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,0,1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,0,0,0,1,0,1,1,0,1,0,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,1,0,0,1,1,1,0,1,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,0,1,1,1,1,1,0,1,1,0,1,1,0,0,1,1,0,1,1,0,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0,1,1,1,1,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,0,0,0,1,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,1,1,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,0,1,0,1,0,1,0,1,0,0,1,0,1,1,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,1,1,1,0,1,1,1,0,1,0,1,1,1,0,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,1,1,0,1,1,0,0,1,1,0,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,1,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,1,1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,1,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,1,0,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,0,0,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,1,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,0,0,1,0,1,1,1,0,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,0,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,0,0,1,1,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,1,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,1,0,1,1,1,0,0,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,1,1,0,1,1,0,0,1,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0,0,0,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,1,1,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,1,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,0,1,1,1,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,1,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,1,1,0,0,0,1,1,1,1,1,1,0,0,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,0,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,1,0,1,0,0,1,1,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,1,0,1,1,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,0,1,1,1,1,1,1,0,0,1,0,0,0,0,1,1,0,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,1,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,1,0,0,1,1,1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,0,1,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,1,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,0,0,0,0,1,1,1,1,1,1,1,0,0,1,0,1,0,1,1,0,0,1,0,1,1,1,1,0,1,1,1,0,0,1,1,0,1,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,0,1,0,1,1,0,0,0,1,0,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,0,1,0,0,1,1,1,1,0,1,1,0,0,1,1,0,1,1,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,1,0,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,1,0,1,1,0,1,1,0,1,0,0,1,0,1,0,0,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,1,1,0,0,0,1,1,1,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,1,1,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,0,0,0,1,1,0,1,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,0,1,1,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,1,0,1,1,1,0,1,1,1,0,1,1,1,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,0,1,1,0,0,0,1,1,0,1,0,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,1,0,1,0,1,0,1,0,1,0,0,0,0,1,1,0,0,1,1,1,0,0,0,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0,0,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,1,1,0,1,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,0,0,0,1,0,1,1,1,1,0,0,0,0,1,0,0,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,0,0,0,0,0,1,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,1,0,0,1,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,0,1,0,0,0,0,1,1,1,0,0,1,1,1,1,0,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,1,0,1,1,0,0,0,1,0,0,1,1,1,1,0,0,0,1,1,1,1,1,1,1,0,1,1,0,1,0,0,1,0,0,1,1,1,1,0,0,1,0,0,1,1,1,0,0,1,0,0,0,1,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,0,1,1,1,0,0,0,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,0,0,0,0,1,1,0,1,0,1,1,0,0,1,0,1,0,0,1,1,0,1,0,0,1,1,1,1,0,1,0,0,1,0,1,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,0,0,1,1,0,1,1,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,0,1,0,1,1,0,0,0,0,0,1,0,1,1,1,0,1,1,0,0,1,1,0,0,0,0,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,0,0,1,1,0,0,1,1,0,0,0,1,1,0,0,0,1,0,0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "def model_2(n_features=1752):\n",
        "    p = 0.05\n",
        "\n",
        "    x_input = layers.Input(shape=(n_features))\n",
        "    \n",
        "    x = layers.Dense(512)(x_input)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(512)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(128)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "    x_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(x_input, x_output, name=\"model_2\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "do_the_all(model=model_2(), epochs=50, batch_size=32, do_plots=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V74AmqHg0-_P",
        "outputId": "6242bdad-e35d-4eed-dfdb-0e6e8aa553fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1752)]            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1024)              1795072   \n",
            "                                                                 \n",
            " layer_normalization_3 (Laye  (None, 1024)             2048      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 768)               787200    \n",
            "                                                                 \n",
            " layer_normalization_4 (Laye  (None, 768)              1536      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 768)               0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 768)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 512)               393728    \n",
            "                                                                 \n",
            " layer_normalization_5 (Laye  (None, 512)              1024      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " layer_normalization_6 (Laye  (None, 512)              1024      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " layer_normalization_7 (Laye  (None, 512)              1024      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,508,481\n",
            "Trainable params: 3,508,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1782/1782 [==============================] - 102s 55ms/step - loss: 0.0947 - cm: 0.9585 - accuracy: 0.9176 - val_loss: 0.1075 - val_cm: 0.9093 - val_accuracy: 0.9350\n",
            "Epoch 2/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0667 - cm: 0.9706 - accuracy: 0.9433 - val_loss: 0.0858 - val_cm: 0.9257 - val_accuracy: 0.9416\n",
            "Epoch 3/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0580 - cm: 0.9743 - accuracy: 0.9509 - val_loss: 0.0811 - val_cm: 0.9330 - val_accuracy: 0.9482\n",
            "Epoch 4/50\n",
            "1782/1782 [==============================] - 100s 56ms/step - loss: 0.0530 - cm: 0.9774 - accuracy: 0.9549 - val_loss: 0.0928 - val_cm: 0.9294 - val_accuracy: 0.9436\n",
            "Epoch 5/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0459 - cm: 0.9809 - accuracy: 0.9601 - val_loss: 0.0842 - val_cm: 0.9384 - val_accuracy: 0.9425\n",
            "Epoch 6/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0427 - cm: 0.9823 - accuracy: 0.9645 - val_loss: 0.0948 - val_cm: 0.9298 - val_accuracy: 0.9494\n",
            "Epoch 7/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0390 - cm: 0.9841 - accuracy: 0.9669 - val_loss: 0.0872 - val_cm: 0.9349 - val_accuracy: 0.9528\n",
            "Epoch 8/50\n",
            "1782/1782 [==============================] - 99s 55ms/step - loss: 0.0355 - cm: 0.9852 - accuracy: 0.9705 - val_loss: 0.0830 - val_cm: 0.9460 - val_accuracy: 0.9504\n",
            "Epoch 9/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0337 - cm: 0.9859 - accuracy: 0.9711 - val_loss: 0.0839 - val_cm: 0.9414 - val_accuracy: 0.9503\n",
            "Epoch 10/50\n",
            "1782/1782 [==============================] - 97s 55ms/step - loss: 0.0321 - cm: 0.9864 - accuracy: 0.9729 - val_loss: 0.0900 - val_cm: 0.9469 - val_accuracy: 0.9515\n",
            "Epoch 11/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0303 - cm: 0.9874 - accuracy: 0.9743 - val_loss: 0.0866 - val_cm: 0.9386 - val_accuracy: 0.9539\n",
            "Epoch 12/50\n",
            "1782/1782 [==============================] - 95s 54ms/step - loss: 0.0292 - cm: 0.9872 - accuracy: 0.9761 - val_loss: 0.0862 - val_cm: 0.9464 - val_accuracy: 0.9523\n",
            "Epoch 13/50\n",
            "1782/1782 [==============================] - 95s 54ms/step - loss: 0.0274 - cm: 0.9877 - accuracy: 0.9765 - val_loss: 0.1103 - val_cm: 0.9345 - val_accuracy: 0.9516\n",
            "Epoch 14/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0268 - cm: 0.9879 - accuracy: 0.9770 - val_loss: 0.0964 - val_cm: 0.9374 - val_accuracy: 0.9544\n",
            "Epoch 15/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0253 - cm: 0.9886 - accuracy: 0.9780 - val_loss: 0.0934 - val_cm: 0.9376 - val_accuracy: 0.9546\n",
            "Epoch 16/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0242 - cm: 0.9895 - accuracy: 0.9789 - val_loss: 0.1067 - val_cm: 0.9309 - val_accuracy: 0.9520\n",
            "Epoch 17/50\n",
            "1782/1782 [==============================] - 97s 54ms/step - loss: 0.0241 - cm: 0.9893 - accuracy: 0.9796 - val_loss: 0.1010 - val_cm: 0.9420 - val_accuracy: 0.9520\n",
            "Epoch 18/50\n",
            "1782/1782 [==============================] - 95s 54ms/step - loss: 0.0233 - cm: 0.9892 - accuracy: 0.9799 - val_loss: 0.0969 - val_cm: 0.9424 - val_accuracy: 0.9549\n",
            "Epoch 19/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0235 - cm: 0.9901 - accuracy: 0.9803 - val_loss: 0.1209 - val_cm: 0.9320 - val_accuracy: 0.9528\n",
            "Epoch 20/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0221 - cm: 0.9899 - accuracy: 0.9810 - val_loss: 0.1233 - val_cm: 0.9397 - val_accuracy: 0.9510\n",
            "Epoch 21/50\n",
            "1782/1782 [==============================] - 97s 55ms/step - loss: 0.0212 - cm: 0.9905 - accuracy: 0.9817 - val_loss: 0.1296 - val_cm: 0.9358 - val_accuracy: 0.9530\n",
            "Epoch 22/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0214 - cm: 0.9903 - accuracy: 0.9818 - val_loss: 0.1058 - val_cm: 0.9413 - val_accuracy: 0.9521\n",
            "Epoch 23/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0206 - cm: 0.9910 - accuracy: 0.9825 - val_loss: 0.1224 - val_cm: 0.9360 - val_accuracy: 0.9525\n",
            "Epoch 24/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0212 - cm: 0.9908 - accuracy: 0.9816 - val_loss: 0.1147 - val_cm: 0.9357 - val_accuracy: 0.9498\n",
            "Epoch 25/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0199 - cm: 0.9912 - accuracy: 0.9826 - val_loss: 0.1332 - val_cm: 0.9357 - val_accuracy: 0.9544\n",
            "Epoch 26/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0193 - cm: 0.9915 - accuracy: 0.9832 - val_loss: 0.1183 - val_cm: 0.9331 - val_accuracy: 0.9534\n",
            "Epoch 27/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0196 - cm: 0.9916 - accuracy: 0.9829 - val_loss: 0.1193 - val_cm: 0.9456 - val_accuracy: 0.9564\n",
            "Epoch 28/50\n",
            "1782/1782 [==============================] - 97s 55ms/step - loss: 0.0190 - cm: 0.9916 - accuracy: 0.9834 - val_loss: 0.1124 - val_cm: 0.9383 - val_accuracy: 0.9526\n",
            "Epoch 29/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0187 - cm: 0.9918 - accuracy: 0.9834 - val_loss: 0.1142 - val_cm: 0.9446 - val_accuracy: 0.9576\n",
            "Epoch 30/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0177 - cm: 0.9924 - accuracy: 0.9842 - val_loss: 0.1533 - val_cm: 0.9289 - val_accuracy: 0.9524\n",
            "Epoch 31/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0177 - cm: 0.9922 - accuracy: 0.9844 - val_loss: 0.1510 - val_cm: 0.9367 - val_accuracy: 0.9523\n",
            "Epoch 32/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0179 - cm: 0.9919 - accuracy: 0.9844 - val_loss: 0.1363 - val_cm: 0.9316 - val_accuracy: 0.9532\n",
            "Epoch 33/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0177 - cm: 0.9919 - accuracy: 0.9842 - val_loss: 0.1294 - val_cm: 0.9429 - val_accuracy: 0.9523\n",
            "Epoch 34/50\n",
            "1782/1782 [==============================] - 97s 55ms/step - loss: 0.0171 - cm: 0.9923 - accuracy: 0.9845 - val_loss: 0.1445 - val_cm: 0.9312 - val_accuracy: 0.9528\n",
            "Epoch 35/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0164 - cm: 0.9922 - accuracy: 0.9851 - val_loss: 0.1491 - val_cm: 0.9384 - val_accuracy: 0.9551\n",
            "Epoch 36/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0166 - cm: 0.9924 - accuracy: 0.9851 - val_loss: 0.1303 - val_cm: 0.9376 - val_accuracy: 0.9561\n",
            "Epoch 37/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0163 - cm: 0.9925 - accuracy: 0.9847 - val_loss: 0.1358 - val_cm: 0.9299 - val_accuracy: 0.9529\n",
            "Epoch 38/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0167 - cm: 0.9924 - accuracy: 0.9849 - val_loss: 0.1245 - val_cm: 0.9409 - val_accuracy: 0.9564\n",
            "Epoch 39/50\n",
            "1782/1782 [==============================] - 97s 54ms/step - loss: 0.0161 - cm: 0.9930 - accuracy: 0.9852 - val_loss: 0.1268 - val_cm: 0.9379 - val_accuracy: 0.9561\n",
            "Epoch 40/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0157 - cm: 0.9930 - accuracy: 0.9853 - val_loss: 0.1387 - val_cm: 0.9397 - val_accuracy: 0.9548\n",
            "Epoch 41/50\n",
            "1782/1782 [==============================] - 95s 53ms/step - loss: 0.0156 - cm: 0.9927 - accuracy: 0.9858 - val_loss: 0.1557 - val_cm: 0.9430 - val_accuracy: 0.9580\n",
            "Epoch 42/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0155 - cm: 0.9928 - accuracy: 0.9861 - val_loss: 0.1271 - val_cm: 0.9428 - val_accuracy: 0.9572\n",
            "Epoch 43/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0157 - cm: 0.9931 - accuracy: 0.9862 - val_loss: 0.1578 - val_cm: 0.9387 - val_accuracy: 0.9548\n",
            "Epoch 44/50\n",
            "1782/1782 [==============================] - 98s 55ms/step - loss: 0.0154 - cm: 0.9925 - accuracy: 0.9865 - val_loss: 0.1444 - val_cm: 0.9356 - val_accuracy: 0.9549\n",
            "Epoch 45/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0149 - cm: 0.9927 - accuracy: 0.9864 - val_loss: 0.1390 - val_cm: 0.9392 - val_accuracy: 0.9553\n",
            "Epoch 46/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0151 - cm: 0.9931 - accuracy: 0.9869 - val_loss: 0.1327 - val_cm: 0.9380 - val_accuracy: 0.9555\n",
            "Epoch 47/50\n",
            "1782/1782 [==============================] - 97s 54ms/step - loss: 0.0148 - cm: 0.9930 - accuracy: 0.9866 - val_loss: 0.1361 - val_cm: 0.9298 - val_accuracy: 0.9526\n",
            "Epoch 48/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0150 - cm: 0.9928 - accuracy: 0.9868 - val_loss: 0.1443 - val_cm: 0.9375 - val_accuracy: 0.9500\n",
            "Epoch 49/50\n",
            "1782/1782 [==============================] - 96s 54ms/step - loss: 0.0149 - cm: 0.9930 - accuracy: 0.9861 - val_loss: 0.1209 - val_cm: 0.9364 - val_accuracy: 0.9536\n",
            "Epoch 50/50\n",
            "1782/1782 [==============================] - 99s 56ms/step - loss: 0.0147 - cm: 0.9936 - accuracy: 0.9872 - val_loss: 0.1443 - val_cm: 0.9399 - val_accuracy: 0.9546\n",
            "\n",
            "Best Epoch: 00010\n",
            "\n",
            "Train Result\n",
            "1782/1782 [==============================] - 26s 14ms/step - loss: 0.0318 - cm: 0.9904 - accuracy: 0.9699\n",
            "\n",
            "Val Result\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0900 - cm: 0.9469 - accuracy: 0.9515\n",
            "\n",
            "Predictions\n",
            "313/313 [==============================] - 7s 20ms/step\n",
            "\n",
            "Number of Viruses: 5067\n",
            "Estimate Score: 0.9867771857114662\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1,1,0,1,1,0,0,1,1,1,1,1,1,0,0,1,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,1,1,1,0,1,1,1,0,0,0,1,0,0,1,1,1,1,1,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,1,0,0,1,0,1,1,0,0,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,0,1,0,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,1,1,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,0,0,1,1,1,0,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,1,1,1,0,0,0,0,1,1,1,0,1,0,1,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,0,0,0,1,0,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,0,1,1,0,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,1,0,1,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,1,0,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,0,1,1,1,0,1,0,0,1,0,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,1,0,1,0,0,0,0,1,1,1,0,0,0,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,0,0,1,0,1,1,1,1,0,0,1,1,1,1,0,1,0,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,1,0,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,1,1,1,0,0,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,1,0,1,1,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,1,1,0,1,1,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,1,0,0,1,1,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,0,0,0,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,1,0,1,1,1,1,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,1,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,1,1,1,0,1,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0,1,0,1,1,1,0,1,1,0,1,0,1,0,0,1,1,1,0,1,1,0,0,0,1,0,0,1,0,0,0,0,0,1,1,0,1,1,0,0,1,0,0,1,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,1,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,1,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,0,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,1,0,1,1,0,0,1,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,0,0,1,1,1,0,0,0,1,1,0,0,1,0,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,0,1,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,1,1,1,1,1,0,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,1,0,1,1,1,0,1,0,0,1,1,1,0,1,1,1,0,0,1,0,1,1,1,0,1,0,0,0,0,1,1,1,1,0,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,1,1,1,0,1,0,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,0,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,0,1,1,0,1,1,1,0,0,1,0,1,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,1,1,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,1,1,1,1,0,0,0,1,0,1,0,0,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,1,1,0,1,1,1,0,0,1,1,1,1,0,1,0,1,1,1,0,1,0,1,0,0,0,1,0,0,0,1,0,1,1,1,1,0,1,0,0,0,0,1,1,1,0,0,1,1,0,1,0,1,1,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,1,0,1,1,0,0,1,0,0,1,0,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,1,1,0,1,0,1,1,1,1,0,0,1,0,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,0,1,0,1,0,1,0,0,1,1,1,0,0,1,1,0,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,0,1,0,1,0,1,1,1,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,0,1,1,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,1,1,0,0,1,1,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,1,0,1,0,0,1,1,0,1,0,0,1,1,1,1,1,1,1,1,0,1,0,1,1,0,1,0,1,1,0,1,1,0,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,1,0,1,1,1,0,1,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,0,1,0,1,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,0,1,0,1,0,1,0,0,0,0,1,0,1,1,0,1,0,1,1,1,0,0,0,0,1,1,0,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,0,1,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,1,1,1,0,1,0,1,1,1,1,0,1,1,1,1,0,0,1,0,0,0,1,1,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,0,0,1,0,1,1,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,1,0,0,0,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,1,1,0,1,1,0,0,1,0,1,1,1,1,1,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,0,0,0,0,0,1,0,0,1,0,1,1,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,1,1,1,0,1,0,1,1,0,1,0,1,0,0,1,0,1,0,0,1,1,1,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,1,0,1,0,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,0,1,0,0,1,0,0,1,1,0,0,0,1,1,0,0,1,1,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,0,1,0,1,0,1,0,0,0,1,1,1,1,0,1,1,0,1,0,0,1,0,0,1,0,1,1,1,1,1,0,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,1,1,0,1,1,1,1,1,0,0,1,1,1,0,0,1,0,1,0,0,1,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,1,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,0,1,1,0,1,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,0,1,1,1,0,0,1,0,1,0,0,0,1,1,0,1,0,0,1,1,0,0,1,1,0,1,1,0,1,1,0,0,0,1,1,0,0,1,0,0,0,1,1,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,1,0,0,1,1,0,1,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,0,0,1,1,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,1,1,1,0,1,0,1,0,1,0,1,1,0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,1,0,1,0,1,0,0,1,0,0,0,1,0,1,0,0,1,0,1,1,0,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,0,1,1,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,1,0,1,0,0,0,1,0,0,0,0,1,1,1,0,0,1,0,1,1,0,0,0,0,1,0,0,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,0,1,0,1,0,0,0,0,1,0,0,1,1,0,1,1,0,0,1,1,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,1,1,1,1,0,1,1,1,0,1,0,0,0,1,0,1,0,0,0,1,1,0,1,0,0,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,1,0,1,1,1,0,1,1,1,0,0,1,1,1,0,1,0,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,0,1,1,0,0,0,1,1,1,1,0,1,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,0,1,0,1,0,1,1,1,0,1,0,0,1,0,0,1,1,1,0,1,0,1,0,1,1,0,1,1,0,0,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1,0,0,0,1,0,1,0,1,0,1,0,0,1,0,1,0,0,0,1,1,1,1,1,1,0,1,0,0,0,1,0,1,1,0,0,0,0,0,0,1,1,1,0,0,0,1,1,1,0,0,1,1,1,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,1,0,1,0,1,1,1,1,0,1,1,1,0,1,0,0,0,0,1,1,1,0,1,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,0,1,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,1,0,1,0,1,1,0,0,1,0,1,0,1,0,1,1,1,0,1,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,1,1,1,1,0,0,0,1,1,0,0,0,1,0,1,1,0,1,0,0,0,1,1,1,1,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1,0,1,0,1,1,1,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,0,1,1,0,0,1,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,0,1,1,1,1,1,1,1,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,0,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,1,0,1,0,1,0,1,0,0,1,0,0,1,1,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0,1,0,1,1,0,1,1,1,0,0,0,1,1,1,0,1,1,0,1,0,0,1,1,1,1,1,1,0,0,1,0,0,1,1,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,1,0,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,1,0,1,1,0,1,0,1,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,1,1,0,1,1,1,0,1,0,1,0,1,1,1,0,0,1,1,1,0,1,1,0,0,1,0,1,1,1,0,1,0,0,1,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,1,0,1,1,1,1,1,0,0,1,1,0,1,0,0,1,0,1,1,0,0,0,0,0,1,0,1,0,0,1,1,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,1,1,1,0,1,0,0,1,1,1,0,1,0,1,0,0,1,0,1,1,0,1,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,1,1,0,1,1,1,1,0,0,1,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,1,0,1,0,1,1,1,1,0,1,0,0,0,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,1,0,0,1,0,1,1,1,1,0,1,1,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,0,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,1,0,0,0,1,1,0,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,0,1,0,1,0,1,1,0,1,1,0,0,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,0,0,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,0,0,0,1,0,0,1,0,1,0,1,1,1,0,0,0,1,0,1,0,1,1,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,1,1,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,1,0,1,0,0,1,1,1,0,1,1,0,1,0,0,1,1,1,1,0,1,1,1,1,0,0,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,0,1,1,1,0,1,1,0,1,0,1,1,0,1,1,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,1,1,0,0,1,1,0,0,1,1,1,1,1,1,0,1,1,0,0,1,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,1,1,1,0,1,1,0,1,1,1,0,0,0,0,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,1,0,0,1,1,0,1,1,0,0,0,1,0,1,0,0,1,1,1,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,1,1,0,1,0,1,0,1,1,0,1,1,1,0,1,0,0,1,1,0,0,0,0,0,0,1,1,1,1,0,1,1,0,1,1,0,1,1,0,1,1,1,1,0,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,0,1,1,1,0,1,1,0,1,0,0,1,0,1,1,1,1,0,1,0,0,0,1,0,1,0,1,1,1,1,0,1,0,0,0,1,1,0,1,0,0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,0,1,0,1,0,1,1,1,0,1,0,1,1,1,0,1,1,1,1,0,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,1,1,1,1,1,0,1,0,1,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,1,0,1,0,0,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,0,0,1,1,1,0,1,1,0,0,0,1,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,1,1,0,0,1,1,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,0,0,1,0,1,1,1,0,1,0,0,0,1,1,1,1,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,0,1,1,0,0,1,1,1,0,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,0,0,1,0,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,1,0,1,1,0,1,1,0,1,0,0,1,0,1,1,0,1,1,0,1,0,1,0,1,1,0,0,1,1,0,1,1,0,0,0,0,0,1,0,1,0,0,0,0,1,0,1,1,1,1,0,1,1,0,0,1,0,0,1,0,0,1,0,1,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,0,0,0,1,1,0,0,1,0,1,1,1,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,0,1,0,1,0,0,1,1,0,0,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,0,1,0,1,1,1,1,1,0,0,1,0,1,0,1,1,1,0,1,1,0,0,1,0,0,0,1,0,1,0,1,1,1,0,0,0,1,1,0,0,0,0,1,1,0,1,1,0,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,1,1,1,0,1,1,1,0,0,0,1,1,0,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,0,1,1,1,0,0,0,1,0,0,0,1,1,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,1,1,1,1,1,0,1,0,0,1,1,0,0,0,1,0,1,1,1,0,0,1,1,0,0,1,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,0,1,1,1,1,1,0,1,1,1,0,1,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0,0,1,1,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,1,0,1,0,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,0,0,0,1,0,1,1,0,1,0,0,1,0,0,0,1,1,1,1,1,1,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,0,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,1,0,0,1,1,1,0,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,0,1,1,1,1,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,1,0,1,1,0,1,1,1,1,0,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,0,0,0,1,0,0,1,0,0,1,1,1,1,0,0,1,1,0,0,1,0,0,0,0,1,0,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,1,1,0,0,0,1,1,0,0,0,1,0,0,0,1,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,1,0,1,0,0,0,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,0,1,0,1,0,1,0,1,0,0,1,0,1,1,0,0,0,0,1,1,0,1,0,1,0,0,1,0,0,0,1,1,0,1,1,1,0,1,0,1,1,1,1,0,0,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,1,0,1,0,0,0,1,0,1,1,1,1,1,0,0,0,1,1,1,0,0,0,1,1,0,1,1,1,0,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,0,0,1,0,0,1,0,1,0,1,1,1,1,0,1,0,0,1,0,1,1,0,1,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,1,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,1,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,0,0,0,0,0,1,0,0,1,1,0,1,0,1,0,0,0,0,0,0,1,1,0,0,1,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,0,0,0,1,1,1,0,1,0,1,1,0,0,0,1,1,0,1,1,1,1,1,0,0,1,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,1,1,0,1,1,1,1,0,1,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,0,0,0,0,0,1,1,1,0,1,1,0,0,1,0,0,1,1,0,1,0,0,0,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,1,0,1,1,1,0,0,1,1,1,1,0,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,0,1,0,0,0,1,1,0,1,1,1,0,1,0,1,0,0,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,0,1,1,1,1,1,0,1,0,0,1,1,0,1,1,0,0,0,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,1,0,1,1,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,1,1,0,1,1,0,1,0,1,0,1,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,1,0,1,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,0,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,0,0,1,1,0,0,1,0,1,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,1,1,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,1,0,0,0,1,1,1,0,1,1,1,0,0,1,0,1,0,1,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,1,1,0,0,1,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0,1,1,0,0,1,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,1,1,0,1,0,0,1,1,0,1,1,1,0,1,0,1,1,1,1,0,0,0,1,1,0,0,1,1,1,1,1,0,1,0,1,1,1,0,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,1,1,0,0,1,1,0,0,0,1,1,0,0,0,1,0,1,0,1,1,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,1,1,0,1,0,1,1,1,0,0,0,1,0,1,0,0,1,1,1,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,1,1,1,1,1,1,0,0,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,1,0,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,1,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,0,1,1,0,1,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,0,1,1,1,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,1,0,1,1,0,1,0,0,0,0,0,1,1,0,0,1,0,0,0,1,1,0,1,0,0,1,0,1,0,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,0,1,0,1,0,0,1,1,0,0,1,0,0,1,1,1,1,0,1,1,1,1,1,1,0,1,1,0,0,0,0,1,1,0,1,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,1,1,1,0,0,1,0,0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,1,0,0,0,0,0,1,1,0,0,0,1,1,1,1,0,0,0,1,0,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,0,0,0,1,1,0,1,1,1,1,0,1,0,1,1,0,0,1,0,0,1,0,1,0,0,0,1,0,1,1,0,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,0,0,1,0,1,0,1,1,0,0,1,0,0,1,1,1,1,1,1,1,0,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,0,1,0,1,1,0,0,0,0,0,1,1,1,1,0,1,1,0,1,1,0,1,1,1,1,1,0,1,0,0,1,1,1,1,0,1,1,0,0,0,0,0,1,1,0,1,0,0,1,0,0,1,0,0,1,0,0,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,1,1,0,1,0,1,0,1,0,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,0,1,0,0,0,1,1,0,0,1,0,1,1,0,1,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,1,1,0,0,0,1,1,1,0,1,1,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,0,1,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,1,0,1,1,0,0,1,0,0,1,1,0,1,1,1,0,0,0,1,1,1,0,0,0,1,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,0,1,0,1,0,0,0,1,1,0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,1,0,0,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,1,0,1,0,1,0,1,1,0,0,0,0,1,1,1,1,0,1,1,0,1,0,0,1,1,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,1,1,0,1,0,0,0,1,0,0,0,1,0,0,0,1,1,0,1,1,0,0,1,1,1,0,0,0,1,0,1,0,1,0,1,1,1,0,1,1,1,0,0,1,1,0,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,1,0,1,1,0,0,0,1,1,0,1,0,0,1,1,1,1,0,1,1,0,0,0,0,1,0,0,0,1,0,1,1,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,1,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,0,0,0,1,0,1,0,0,1,1,1,0,1,1,1,0,1,1,1,1,0,0,0,1,0,0,0,1,0,1,1,0,0,0,1,1,0,1,1,1,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,1,1,1,0,1,1,1,0,1,0,0,0,0,1,0,1,1,0,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,0,1,0,0,0,0,1,1,0,1,0,1,1,0,1,0,0,1,0,0,0,1,0,0,1,0,0,0,1,1,0,1,0,1,1,1,1,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,0,0,0,0,0,0,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,0,1,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,1,0,0,0,1,0,1,0,0,1,1,0,0,1,0,0,1,0,0,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,0,1,1,0,1,0,0,1,0,0,1,1,1,1,0,0,1,0,0,1,1,1,0,0,1,0,0,0,1,0,1,1,1,0,1,1,0,0,0,0,1,0,0,1,1,1,0,0,1,0,1,1,1,1,1,0,1,1,1,0,0,0,0,1,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,0,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,0,1,0,0,0,1,0,1,0,1,1,0,1,1,1,0,0,0,0,0,1,1,0,1,1,1,1,0,0,1,0,1,0,0,1,1,0,1,1,0,1,1,1,1,0,1,0,0,1,0,1,1,0,0,1,1,1,1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,0,1,0,1,0,0,0,0,1,0,0,1,1,1,1,0,1,0,1,0,0,0,1,1,0,1,1,0,1,0,1,1,0,1,0,0,1,1,0,0,1,0,1,1,0,0,1,0,0,0,1,1,1,1,0,1,0,1,1,0,0,0,0,0,1,0,1,1,1,0,1,1,0,0,1,1,0,0,0,0,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,1,1,0,1,1,1,1,0,1,1,1,1,0,0,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,0,0,1,1,0,0,1,1,1,0,0,1,1,0,0,0,1,0,0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "def model_3(n_features=1752):\n",
        "    p = 0.05\n",
        "\n",
        "    x_input = layers.Input(shape=(n_features))\n",
        "    \n",
        "    x = layers.Dense(1024)(x_input)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(768)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(512)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(512)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x = layers.Dense(512)(x)\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation('elu')(x)\n",
        "    x = layers.Dropout(p)(x)\n",
        "\n",
        "    x_output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(x_input, x_output, name=\"model_3\")\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "do_the_all(model=model_3(), epochs=50, batch_size=32, do_plots=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMvOb0sm_zvo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}