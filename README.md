# Malware vs. Benign Classification

## Project Overview

This project involves the classification of malware samples against benign (non-malicious) samples using deep learning models, with a focus on Gradient Boosted Decision Trees and deep neural networks. The primary goal is to achieve high accuracy in distinguishing between these two classes, with a focus on using Gradient Boosted Decision Trees and deep neural networks.

## Project Structure

The project is divided into two main files, each addressing a different approach to the classification task.

### File 1: Gradient Boosted Decision Trees

#### Imports
- We start by importing the necessary libraries, including LightGBM, NumPy, and Scikit-learn utilities.

#### Dataset
- We load the dataset using Google Drive links and create training and evaluation datasets for malware and benign samples.
- The dataset consists of features extracted from the samples.
- We print the shapes of these datasets to provide insights into the data dimensions.

#### Model
- We define a function `do_the_all` to perform various operations:
  - Training a LightGBM model with specified hyperparameters.
  - Saving the best model based on validation performance.
  - Making predictions on the evaluation dataset.

#### Parameters
- We define hyperparameters for the LightGBM model, including learning rate, tree depth, and evaluation metrics such as AUC, average precision, binary log loss, and binary error.

#### Validation and Test
- We calculate the optimal threshold for classification using precision-recall curves and select the best threshold based on custom scoring.
- We evaluate the model on the validation set and report precision, recall, F1-score, and custom scoring.
- We also make predictions on the test dataset and report the number of viruses detected.

#### Playground
- This section provides some example performance metrics and calculations, showcasing the effectiveness of the model.

### File 2: Deep Neural Network

#### Imports
- We start by importing the required libraries for building and training deep neural networks using Keras and TensorFlow.

#### Dataset
- Similar to File 1, we load the dataset using Google Drive links and create training and evaluation datasets for malware and benign samples.
- Data preprocessing involves standardization by subtracting the mean and dividing by the standard deviation.

#### Models
- We define three different neural network architectures (`model_1`, `model_2`, and `model_3`) with varying layer structures and regularization techniques.

#### Training and Evaluation
- We define functions for training and evaluating neural network models.
- Training includes custom weighted binary cross-entropy loss to handle class imbalance.
- The `compile_and_fit` function trains the model, saves the best-performing model checkpoint, and provides evaluation results.

#### Model Evaluation
- We evaluate the trained models on the training and validation sets, reporting accuracy and custom scoring.
- The models are also used to make predictions on the evaluation dataset, with the number of viruses detected being reported.

## Running the Code

To run this project, follow these steps:

1. Ensure you have all the necessary libraries and dependencies installed, including LightGBM, Keras, TensorFlow, NumPy, and Scikit-learn.

2. Download the dataset files specified in the code and place them in the appropriate file paths.

3. Run the code in both files separately to train and evaluate the models. The results and performance metrics will be displayed in the console.

4. Experiment with different hyperparameters, model architectures, and training settings to optimize the classification performance.

## License

This project is licensed under the [MIT License](LICENSE).

## Conclusion

This project presents two distinct approaches to the classification of malware versus benign samples, utilizing both Gradient Boosted Decision Trees and deep neural networks. Each approach provides valuable insights into addressing this classification challenge, offering distinct advantages and considerations. 

Throughout the project, it became evident that the Gradient Boosted Decision Trees model outperformed other methods in terms of accuracy and precision. The decision to choose one approach over the other may depend on specific project requirements and constraints. 

For future work, there is ample room for further exploration and fine-tuning of the models to achieve even more remarkable classification results. Continued optimization and experimentation with hyperparameters, feature engineering, and data preprocessing techniques could lead to enhanced performance in real-world applications.
